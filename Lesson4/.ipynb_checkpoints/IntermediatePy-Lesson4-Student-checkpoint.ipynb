{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Lecture 4 - Clustering Data and Heatmaps\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python's `statsmodels` function can be used for performing regression\n",
    "* Regression is helpful when you want to perform `feature selection` and/or `classification`\n",
    "* Generalized Linear Models (`GLM`) are regression models with relaxed assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) [Introduction to clustering](#1)\n",
    "#### (2) [Distance measures](#2)\n",
    "#### (3) [Clustering algorithms](#3)\n",
    "#### (4) [Application to real data](#4)\n",
    "#### (5) [Visualizing with clustermaps](#5)\n",
    "#### (6) [Challenges](#6)\n",
    "#### (7) [Extra resources](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Clustering <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Liang-Wong et. al., Calico Life Sciences LLC, 2019 (eLIFE)](https://elifesciences.org/articles/42940)\n",
    "<img src=\"img/tsne_clust.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We won't focus on `tSNE` in this lesson, but if you are interested...\n",
    "`tSNE` is another method for dimensionality reduction, which is similar to `PCA` except it does not preserve distances between data points. You can read more about `tSNE` [here](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm). In practice, it is very effective for single cell data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are the points above being colored?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction can be used to visualize our data, but it doesn't actually define clusters. Those are defined by clustering algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets create some toy data to introduce these concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=500, centers=4,\n",
    "                       cluster_std=0.6, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\"> How many clusters do we have? </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering relies on distance (or similarity) between points\n",
    "<!-- -->     |     __Distance__     |  __Similarity__ |\n",
    "-------------|:--------------------:|:---------------:|\n",
    "__Range__    |   $[0,\\infty)$      | $[0,1]$         |\n",
    "__Examples__  | _Euclidian_, _Hamming_,<br>_Cosine_, etc. |  _Correlation_, _Cosine_, etc.  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "If data is standardized: Similarity = 1 - Distance\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "> ?pairwise_distances\n",
    "```\n",
    "```\n",
    "Valid values for metric are:\n",
    "\n",
    "- From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
    "  'manhattan']. These metrics support sparse matrix inputs.\n",
    "\n",
    "- From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
    "  'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
    "  'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
    "  'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
    "  See the documentation for scipy.spatial.distance for details on these\n",
    "  metrics. These metrics do not support sparse matrix inputs.\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Euclidean Distance`\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*n6kmkzjKVTOWeXDxsx2daQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euc_dist = pairwise_distances(X,metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Euclidean Info\")\n",
    "print(\"---------------\")\n",
    "print(\"Min Distance: {}\".format(euc_dist.min()))\n",
    "print(\"Max Distance: {}\".format(euc_dist.max()))\n",
    "print(\"Average Distance: {}\".format(euc_dist.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(euc_dist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\"> Group Exercise: How can we order our data such that pairs with lower distances are closer to each other?   </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euc_dist_sort = pairwise_distances(X_sorted,metric=\"euclidean\")\n",
    "plt.imshow(euc_dist_sort)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Clustering Algorithms <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `kmeans`\n",
    "<img src=\"https://assets.yihui.name/figures/animation/example/kmeans-ani/demo-c.mp4?dl=1\" width=\"350\" height=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4,random_state=0)\n",
    "preds_kmeans = kmeans.fit_predict(X)\n",
    "preds_kmeans[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=preds_kmeans,cmap=\"viridis\")\n",
    "plt.scatter(centers[:,0], centers[:,1],color=\"gray\",s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `hierarchical`\n",
    "\n",
    "Introduces an additional concept called `linkage`:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Pamela_Guevara/publication/281014334/figure/fig57/AS:418517879934980@1476793847581/The-three-linkage-types-of-hierarchical-clustering-single-link-complete-link-and.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "Visualization for hierarchical clustering:\n",
    "\n",
    "| Dendogram             |  Plot |\n",
    "|-------------------------|-------------------------|\n",
    "|<img src=\"https://people.revoledu.com/kardi/tutorial/Clustering/image/Numerical%20Example_clip_image040.jpg\" width=\"200\" height=\"200\">  | <img src=\"https://people.revoledu.com/kardi/tutorial/Clustering/image/Numerical%20Example_clip_image042.jpg\" width=\"200\" height=\"200\">|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = linkage(X,method=\"ward\") # output is matrix that is used to contruct tree. Columns 1 and 2 defines new combined cluster at the ith iteration, Column 3 has distance between clusters, and Column 4 has the number of observations in each cluster   \n",
    "dendrogram(linked)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hclust = AgglomerativeClustering(n_clusters=4,linkage=\"ward\",)\n",
    "preds_hclust = hclust.fit_predict(X)\n",
    "preds_hclust[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=preds_hclust,cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\"> Exercise: What happens when you change the linkage for hierarchical clustering? </h3>\n",
    "\n",
    "Try `single`, `complete`, and `average` linkages with `n_clusters=4`. You can overwrite the above function **OR**...\n",
    "\n",
    "For a **challenge** try these two extensions:\n",
    "1. Prevent copying and pasting multiple times by writing a function that takes **linkage type** and **the data** as input and outputs **predictions**. (_You can also output the plot if you would like!_)\n",
    "\n",
    "\n",
    "2. Use `plt.subplots` to combine the scatter plots for each linkage type into one main plot. We haven't shown you how to use this function in the course yet, but if you type `?plt.subplots` and scroll down there are helpful examples! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_preds_hclust(data,link_type):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this line if you want to combine the plots together\n",
    "# fig,axes = plt.subplots(nrows=1,ncols=3,figsize=(10,4))\n",
    "\n",
    "return_preds_hclust(X,\"single\")\n",
    "return_preds_hclust(X,\"complete\")\n",
    "return_preds_hclust(X,\"average\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\"> Which clustering algorithm should you use?   </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with each:\n",
    "\n",
    "`hierarchical`\n",
    "* Takes more time, not great for large datasets\n",
    "* Extra linkage parameter\n",
    "\n",
    "`kmeans`\n",
    "* Not as reproducible (dependent on choice of centers)\n",
    "* Requires you to choose expected number of clusters\n",
    "* Best used for hyperspherical clusters\n",
    "* Can only be used with Euclidian distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Real Data <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_data_meta = pd.read_csv(\"inSphero.abundance.table_edit190410_no_outlier.csv\",index_col=0)\n",
    "expr_data = expr_data_meta.drop(\"symbol\",axis=1)\n",
    "annots = expr_data_meta.loc[:,[\"symbol\"]]\n",
    "description = pd.read_csv(\"inSphero_doe_edit190410.csv\",index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expr_data_t = expr_data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\"> Exercise: Plot heatmap of euclidean distance between samples  </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use the `pairwise_distances()` and `plt.imshow` functions from before. \n",
    "\n",
    "Be mindful of which data to use (`expr_data` or `expr_data_t`). Make sure to check expected input for functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_expr = KMeans(n_clusters=4,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_kmeans_expr = kmeans_expr.fit_predict(expr_data_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preds_kmeans_expr,index=expr_data_t.index,columns=[\"label_kmeans\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = linkage(expr_data_t, method='ward',metric=\"euclidean\")\n",
    "labels = expr_data.columns\n",
    "plt.figure(figsize=(6, 4))  \n",
    "dendrogram(linked,  \n",
    "            orientation='top',\n",
    "            labels=labels,\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hclust_expr = AgglomerativeClustering(n_clusters=4,linkage=\"ward\",affinity=\"euclidean\")\n",
    "preds_hclust_expr = hclust_expr.fit_predict(expr_data_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preds_hclust_expr,index=expr_data_t.index,columns=[\"label_hclust\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing with Clustermaps <a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clust_out = sns.clustermap(expr_data,vmin=-3,vmax=6,center=0,cmap=\"bwr\",)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problems <a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when you use different distance metrics for hierarchical clustering our real data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: You can visualize distance with `pairwise_distances()` and `plt.imshow()`. Change the `affinity` parameter in `AgglomerativeClustering()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Clustering accuracy is dependent on the distance measure used!!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create contingency or confusion matrix to compare true and predicted cluster pairs with either the toy or real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "For fun you can also make a heatmap. This will also give us insight into which clusters numbers correspond to each other.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize similarity between kmeans and hierarchical cluster outputs using rand index (see below) with either the toy or real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Resources <a class=\"anchor\" id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating cluster validaty\n",
    "\n",
    "#### Externally (true labels are known)\n",
    "* [contingency table](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cluster.contingency_matrix.html)\n",
    "* [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)\n",
    "* [rand index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)\n",
    "\n",
    "#### Internally (true labels are unknown)\n",
    "* [silhouette coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)\n",
    "\n",
    "#### Relativally (comparing two different clusterings)\n",
    "* [jaccard score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html#sklearn.metrics.jaccard_score)\n",
    "* [rand index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)\n",
    "\n",
    "### Sklearn clustering [documentation](https://scikit-learn.org/stable/modules/clustering.html) with additional metrics.\n",
    "\n",
    "### [Other Clustering Algorithms](https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Indepth Illustration of K-means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Is k-means reproducible? \n",
    "**Hint:** try changing the random seed for the kmeans algorithm. Some good ones are `random_state=30` and `random_state=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating data with less samples and more clusters\n",
    "X_noise, y_true_noise = make_blobs(n_samples=200, centers=5,\n",
    "                       cluster_std=0.5, random_state=0)\n",
    "\n",
    "# Creating subplot figure\n",
    "fig, ax = plt.subplots(3,3,figsize=(20,10),sharey=True,sharex=True)\n",
    "ax = ax.ravel()\n",
    "\n",
    "# Plotting raw, un-clustered data\n",
    "ax[0].scatter(X_noise[:,0],X_noise[:,1],color=\"black\")\n",
    "ax[0].set_title(\"Raw Data\",fontsize=20)\n",
    "\n",
    "# Want to stop kmeans at a max number of iterations (it) so we are going to try a range\n",
    "for it in range(1,8):\n",
    "    \n",
    "    # Run KMeans, choose centers randomly, only let algorithm run for max number of iterations\n",
    "    kmeans_iter = KMeans(n_clusters=5,random_state=30,init=\"random\",n_init=1,max_iter=it) # Update the random state here\n",
    "    preds_kmeans_iter = kmeans_iter.fit_predict(X_noise)\n",
    "    centers = kmeans_iter.cluster_centers_\n",
    "    \n",
    "    # Plot the data and color by the current predictions at this iteration\n",
    "    ax[it].scatter(X_noise[:,0],X_noise[:,1],c=preds_kmeans_iter,cmap=\"viridis\")\n",
    "    ax[it].scatter(centers[:,0], centers[:,1],color=\"gray\",s=200)\n",
    "    ax[it].set_title(\"Iteration = {}\".format(it),fontsize=20)\n",
    "\n",
    "# Also plot the true labels\n",
    "ax[-1].scatter(X_noise[:,0],X_noise[:,1],c=y_true_noise,cmap=\"viridis\")\n",
    "ax[-1].set_title(\"True Labels\",fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
